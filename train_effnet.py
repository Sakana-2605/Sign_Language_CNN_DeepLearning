import os
import tensorflow as tf
from tensorflow.keras.applications import EfficiencyNetB0
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# --- 1. C·∫§U H√åNH GPU (QUAN TR·ªåNG) ---
# ƒêo·∫°n n√†y gi√∫p tr√°nh l·ªói tr√†n b·ªô nh·ªõ (OOM) tr√™n Windows
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"‚úÖ ƒê√É K√çCH HO·∫†T GPU: {len(gpus)} thi·∫øt b·ªã.")
        print(f"Chi ti·∫øt: {gpus}")
    except RuntimeError as e:
        print(f"L·ªói c·∫•u h√¨nh GPU: {e}")
else:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y GPU, qu√° tr√¨nh train s·∫Ω r·∫•t ch·∫≠m tr√™n CPU!")

# --- 2. C·∫§U H√åNH PARAMETERS ---
DATA_DIR = './data_augmented' 
IMG_SIZE = (224, 224) # K√≠ch th∆∞·ªõc chu·∫©n t·ªëi ∆∞u cho EfficiencyNet-B0
BATCH_SIZE = 32       # N·∫øu b·ªã l·ªói OOM, h√£y gi·∫£m xu·ªëng 16
EPOCHS = 50
LEARNING_RATE = 0.001 # B·∫Øt ƒë·∫ßu v·ªõi LR cao h∆°n m·ªôt ch√∫t v√¨ ta d√πng ReduceLROnPlateau
MODEL_SAVE_PATH = 'models/sign_language_effnet.h5'

os.makedirs('models', exist_ok=True)

def build_model(num_classes):
    # Kh·ªüi t·∫°o base model v·ªõi tr·ªçng s·ªë ImageNet
    base_model = EfficiencyNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    
    # B∆∞·ªõc ƒë·∫ßu: ƒê√≥ng bƒÉng base model
    base_model.trainable = False 
    
    inputs = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    
    # L∆∞u √Ω: EfficiencyNetB0 ƒê√É T√çCH H·ª¢P s·∫µn l·ªõp Rescaling (0-255 -> 0-1) b√™n trong. 
    # Kh√¥ng c·∫ßn preprocess_input nh∆∞ MobileNetV2.
    x = base_model(inputs, training=False)
    
    x = GlobalAveragePooling2D()(x) 
    x = Dropout(0.3)(x) # TƒÉng dropout m·ªôt ch√∫t ƒë·ªÉ tr√°nh overfitting
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_classes, activation='softmax')(x) 
    
    model = Model(inputs, outputs)
    return model

def main():
    # --- 3. LOAD D·ªÆ LI·ªÜU ---
    train_ds = tf.keras.utils.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )
    
    val_ds = tf.keras.utils.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )

    class_names = train_ds.class_names
    num_classes = len(class_names)
    
    # L∆∞u nh√£n ƒë·ªÉ d√πng sau n√†y
    with open('models/classes.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(class_names))

    # T·ªëi ∆∞u Pipeline
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

    # --- 4. CALLBACKS (B·ªò N√ÉO C·ª¶A QU√Å TR√åNH TRAIN) ---
    callbacks = [
        # D·ª´ng n·∫øu val_loss kh√¥ng gi·∫£m sau 6 epoch
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),
        # Gi·∫£m LR n·∫øu model b·ªã ch·ªØng l·∫°i (gi√∫p h·ªôi t·ª• s√¢u h∆°n)
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6),
        # L∆∞u model t·ªët nh·∫•t trong qu√° tr√¨nh ch·∫°y
        tf.keras.callbacks.ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True)
    ]

    # --- 5. TRAINING ---
    model = build_model(num_classes)
    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    print("\nüöÄ Giai ƒëo·∫°n 1: Train l·ªõp ph√¢n lo·∫°i cu·ªëi c√πng...")
    model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)

    # --- 6. FINE-TUNING (N√ÇNG CAO) ---
    print("\nüöÄ Giai ƒëo·∫°n 2: Fine-tuning m·ªôt ph·∫ßn Base Model...")
    # M·ªü kh√≥a to√†n b·ªô model
    for layer in model.layers:
        if isinstance(layer, Model): # Ch√≠nh l√† base_model
            layer.trainable = True
            # ƒê√≥ng bƒÉng l·∫°i c√°c l·ªõp ƒë·∫ßu (v√≠ d·ª•: ch·ªâ m·ªü 30 l·ªõp cu·ªëi)
            for l in layer.layers[:-30]:
                l.trainable = False

    # Compile l·∫°i v·ªõi LR c·ª±c nh·ªè
    model.compile(optimizer=Adam(learning_rate=1e-5),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks)

    print(f"\n‚úÖ Ho√†n t·∫•t! Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()